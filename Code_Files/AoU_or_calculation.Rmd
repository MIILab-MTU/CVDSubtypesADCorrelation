---
title: "CVD AD Odds Ratio"
author: "Aili Toyli"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This will be a logistic regression model predicting the odds of developing AD with the each of the various subtypes of CVD and sex as factors.

```{r}
#read in necessary libraries
library(tidyverse)

```

##Load in AD/CVD data for correct cohort
```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "AD_CVD_diagnoses" for domain "observation" and was generated for All of Us Controlled Tier Dataset v7
dataset_24103539_observation_sql <- paste("
    SELECT
        observation.person_id,
        o_source_concept.concept_name as source_concept_name,
        o_source_concept.concept_code as source_concept_code,
        o_source_concept.vocabulary_id as source_vocabulary 
    FROM
        ( SELECT
            * 
        FROM
            `observation` observation 
        WHERE
            (
                observation_source_concept_id IN (44830102, 45552870)
            )  
            AND (
                observation.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `cb_search_person` p 
                    WHERE
                        has_ehr_data = 1 ) )
            )) observation 
    LEFT JOIN
        `concept` o_source_concept 
            ON observation.observation_source_concept_id = o_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
observation_24103539_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "observation_24103539",
  "observation_24103539_*.csv")
message(str_glue('The data will be written to {observation_24103539_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_24103539_observation_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  observation_24103539_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {observation_24103539_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_24103539_observation_df <- read_bq_export_from_workspace_bucket(observation_24103539_path)

dim(dataset_24103539_observation_df)

head(dataset_24103539_observation_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "AD_CVD_diagnoses" for domain "condition" and was generated for All of Us Controlled Tier Dataset v7
dataset_24103539_condition_sql <- paste("
    SELECT
        c_occurrence.person_id,
        c_source_concept.concept_name as source_concept_name,
        c_source_concept.concept_code as source_concept_code,
        c_source_concept.vocabulary_id as source_vocabulary 
    FROM
        ( SELECT
            * 
        FROM
            `condition_occurrence` c_occurrence 
        WHERE
            (
                condition_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (312327, 313217, 315286, 316139, 320128, 321318, 378419, 443454)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1) 
                OR  condition_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (1569114, 1569115, 1569116, 1569133, 1569134, 1569135, 1569145, 1569162, 1569419, 1569420, 35207643, 35207644, 35207645, 35207646, 35207647, 35207648, 35207649, 35207650, 35207651, 35207652, 35207653, 35207654, 35207655, 35207656, 35207657, 35207658, 35207659, 35207660, 35207661, 35207662, 35207663, 35207664, 35207665, 35207666, 35207667, 35207702, 35207703, 35207704, 35207705, 35207706, 35207766, 35207767, 35207768, 35207769, 35207770, 35207771, 35207915, 35207916, 35207917, 35207918, 44819738, 44821971, 44823135, 44826658, 44832409, 44833593, 44833594, 45533439, 45538368, 45538373, 45538374, 45538375, 45538377, 45543167, 45543168, 45548010, 45548011, 45548012, 45548013, 45552792, 45552871, 45557538, 45557539, 45562343, 45562344, 45562351, 45567167, 45567168, 45576866, 45576868, 45581766, 45581775, 45586572, 45586573, 45586574, 45586575, 45591456, 45591458, 45591459, 45591460, 45591461, 45591537, 45596197, 45596198, 45596199, 45596204, 45601024,
 45601026, 45601027, 45601028, 45605777, 45605784, 45605785, 45605786, 45605787, 45605788)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )  
            AND (
                c_occurrence.PERSON_ID IN (SELECT
                    distinct person_id  
                FROM
                    `cb_search_person` cb_search_person  
                WHERE
                    cb_search_person.person_id IN (SELECT
                        person_id 
                    FROM
                        `cb_search_person` p 
                    WHERE
                        has_ehr_data = 1 ) )
            )) c_occurrence 
    LEFT JOIN
        `concept` c_source_concept 
            ON c_occurrence.condition_source_concept_id = c_source_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
condition_24103539_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "condition_24103539",
  "condition_24103539_*.csv")
message(str_glue('The data will be written to {condition_24103539_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_24103539_condition_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  condition_24103539_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {condition_24103539_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(source_concept_name = col_character(), source_concept_code = col_character(), source_vocabulary = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_24103539_condition_df <- read_bq_export_from_workspace_bucket(condition_24103539_path)

dim(dataset_24103539_condition_df)

head(dataset_24103539_condition_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "AD_CVD_diagnoses" for domain "person" and was generated for All of Us Controlled Tier Dataset v7
dataset_24103539_person_sql <- paste("
    SELECT
        person.person_id,
        person.birth_datetime as date_of_birth 
    FROM
        `person` person   
    WHERE
        person.PERSON_ID IN (SELECT
            distinct person_id  
        FROM
            `cb_search_person` cb_search_person  
        WHERE
            cb_search_person.person_id IN (SELECT
                person_id 
            FROM
                `cb_search_person` p 
            WHERE
                has_ehr_data = 1 ) )", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
person_24103539_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "person_24103539",
  "person_24103539_*.csv")
message(str_glue('The data will be written to {person_24103539_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_24103539_person_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  person_24103539_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {person_24103539_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- NULL
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_24103539_person_df <- read_bq_export_from_workspace_bucket(person_24103539_path)

dim(dataset_24103539_person_df)

head(dataset_24103539_person_df, 5)
```
## Extract Diagnoses
```{r}
ad_cvd <- rbind(dataset_24103539_condition_df, dataset_24103539_observation_df) %>% select(source_concept_code, everything())

#initialize list
icd_codes <- list(

#save all icd10 codes in a list
Code = list( a = 'I10',
             b = 'I95',
             c = 'I20',
             d = 'I21',
             e = 'I26',
             f = 'I48',
             g = 'I50',
             h = 'I44',
             i = c('I05', 'I06', 'I07', 'I08', 'I09'),
             j = 'I25',
             k = 'I63',
             l = 'G30'
),

#Save disease names in second element of list
Disease = c('Hypertension', 'Hypotension', 'Angina.Pectoris', 'Acute.Myocardial.Infarction', 'Pulmonary.Embolism', 'Atrial.Fibriliation', 'Heart.Failure', 'Blockage', 'Chronic.Rheumatic.Heart.Disease', 'Chronic.Ischemic.Heart.Disease', 'Cerebral.Infarction', 'Alzheimers.Disease')

)

#function to search for ICD 10 diagnosis of AD or CVD
icd_lookup <- function(icd10_codes, data, file_name, output_dir = "C:/Users/agtoy/Desktop/URIP/UKB Cleaned Data") {
  # Ensure necessary packages are loaded
  library(tidyverse)
  library(stringr)
  
  # Check that `icd10_codes` has the correct structure
  if (!is.list(icd10_codes) || length(icd10_codes) != 2) {
    stop("icd10_codes must be a list with two elements: numerical codes and disease names.")
  }
  
  # Ensure the data has at least one column
  if (ncol(data) < 1) {
    stop("The data frame must have at least one column.")
  }
  
  # Iterate through the ICD-10 codes
  for (i in seq_along(icd10_codes[[1]])) {
    # Check if current code exists in the data
    code_matches <- sapply(icd10_codes[[1]][[i]], function(p) grepl(p, data[[1]]))
    data[[icd10_codes[[2]][[i]]]] <- apply(code_matches, 1, any)
  }
  
  # Save the updated data frame to the specified directory
  if (!dir.exists(output_dir)) {
    stop("The specified output directory does not exist.")
  }
  write.csv(data, file.path(output_dir, paste0(file_name, ".csv")), row.names = FALSE)
}

icd_lookup(icd_codes, ad_cvd, 'ad_cvd_data', "~/")

cvd <- read.csv("~/ad_cvd_data.csv")
cvd <- cvd %>% 
  select(-source_concept_code, -source_vocabulary, -source_concept_name) %>% group_by(person_id) %>% 
  summarise(across(everything(), sum)) %>% 
    mutate(across(-person_id, ~ .x != 0))

#join with the demographics dataframe
cvd <- cvd %>% 
  right_join(dataset_24103539_person_df) %>% 
  mutate(across(everything(), ~ replace(.x, is.na(.x), FALSE)))
```


##Load other covariates
```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "Covars" for domain "condition" and was generated for All of Us Controlled Tier Dataset v7
dataset_67990468_condition_sql <- paste("
    SELECT
        c_occurrence.person_id,
        c_standard_concept.concept_name as standard_concept_name,
        c_status.concept_name as condition_status_concept_name 
    FROM
        ( SELECT
            * 
        FROM
            `condition_occurrence` c_occurrence 
        WHERE
            (
                condition_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (440383)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 1 
                    AND is_selectable = 1) 
                OR  condition_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (1326492, 1326493, 1567956, 1567958, 1567959, 1567960, 1567964, 1567965, 1567966, 1567969, 1567971, 35206881, 35206882, 37200198, 37200199, 37200200, 37200201, 37200202, 37200203, 37200204, 37200205, 37200206, 37200207, 37200208, 37200209, 37200210, 37200211, 37200212, 37200213, 37200214, 37200215, 37200216, 37200217, 37200218, 37200219, 37200220, 37200221, 37200222, 37200223, 37200224, 37200225, 37200227, 37200228, 37200229, 37200230, 37200232, 37200233, 37200234, 37200235, 37200237, 37200238, 37200239, 37200240, 37200242, 37200243, 37200244, 37200245, 37200246, 37200247, 37200248, 37200249, 37200251, 37200252, 37200253, 37200254, 45533019, 45533020, 45533021, 45533022, 45533023, 45537961, 45537962, 45542738, 45547625, 45547626, 45547627, 45552385, 45552386, 45557112, 45557113, 45561949, 45566731, 45576443, 45581352, 45581353, 45581354, 45581355, 45586139, 45586140, 45591027, 45591029, 45591030, 45591031, 45595797, 45595798, 45595799, 45600641,
 45600642, 45605401, 45605402, 45605403, 45605404, 45605405)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )) c_occurrence 
    LEFT JOIN
        `concept` c_standard_concept 
            ON c_occurrence.condition_concept_id = c_standard_concept.concept_id 
    LEFT JOIN
        `concept` c_status 
            ON c_occurrence.condition_status_concept_id = c_status.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
condition_67990468_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "condition_67990468",
  "condition_67990468_*.csv")
message(str_glue('The data will be written to {condition_67990468_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_67990468_condition_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  condition_67990468_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {condition_67990468_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character(), condition_status_concept_name = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_67990468_condition_df <- read_bq_export_from_workspace_bucket(condition_67990468_path)

dim(dataset_67990468_condition_df)

head(dataset_67990468_condition_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "Covars" for domain "survey" and was generated for All of Us Controlled Tier Dataset v7
dataset_67990468_survey_sql <- paste("
    SELECT
        answer.person_id,
        answer.question,
        answer.answer  
    FROM
        `ds_survey` answer   
    WHERE
        (
            question_concept_id IN (1585375, 1585873, 1585940, 1586140, 1586201)
        )", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
survey_67990468_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "survey_67990468",
  "survey_67990468_*.csv")
message(str_glue('The data will be written to {survey_67990468_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_67990468_survey_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  survey_67990468_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {survey_67990468_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(survey = col_character(), question = col_character(), answer = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_67990468_survey_df <- read_bq_export_from_workspace_bucket(survey_67990468_path)

dim(dataset_67990468_survey_df)

head(dataset_67990468_survey_df, 5)
library(tidyverse)
library(bigrquery)

# This query represents dataset "Covars" for domain "measurement" and was generated for All of Us Controlled Tier Dataset v7
dataset_67990468_measurement_sql <- paste("
    SELECT
        measurement.person_id,
        m_standard_concept.concept_name as standard_concept_name,
        measurement.value_as_number 
    FROM
        ( SELECT
            * 
        FROM
            `measurement` measurement 
        WHERE
            (
                measurement_source_concept_id IN (SELECT
                    DISTINCT c.concept_id 
                FROM
                    `cb_criteria` c 
                JOIN
                    (SELECT
                        CAST(cr.id as string) AS id       
                    FROM
                        `cb_criteria` cr       
                    WHERE
                        concept_id IN (903124)       
                        AND full_text LIKE '%_rank1]%'      ) a 
                        ON (c.path LIKE CONCAT('%.', a.id, '.%') 
                        OR c.path LIKE CONCAT('%.', a.id) 
                        OR c.path LIKE CONCAT(a.id, '.%') 
                        OR c.path = a.id) 
                WHERE
                    is_standard = 0 
                    AND is_selectable = 1)
            )) measurement 
    LEFT JOIN
        `concept` m_standard_concept 
            ON measurement.measurement_concept_id = m_standard_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
measurement_67990468_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "measurement_67990468",
  "measurement_67990468_*.csv")
message(str_glue('The data will be written to {measurement_67990468_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_67990468_measurement_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  measurement_67990468_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {measurement_67990468_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(standard_concept_name = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_67990468_measurement_df <- read_bq_export_from_workspace_bucket(measurement_67990468_path)

dim(dataset_67990468_measurement_df)

head(dataset_67990468_measurement_df, 5)
```

##Data cleaning on covariates
```{r}
condition_covars <- dataset_67990468_condition_df

head(condition_covars)

#extract correct labeling for depression and type 2 diabetes diagnoses
condition_covars <- condition_covars %>%
  rename(diagnosis = standard_concept_name) %>% 
  mutate(diagnosis = if_else(str_detect(diagnosis, regex("Depress", ignore_case = TRUE))|str_detect(diagnosis, regex("dysthymia", ignore_case = TRUE))|str_detect(diagnosis, regex("seasonal", ignore_case = TRUE))|str_detect(diagnosis, regex("dysphor", ignore_case = TRUE))|str_detect(diagnosis, regex("blues", ignore_case = TRUE)),
                             "Depression",
                             diagnosis))%>% 
  mutate(diagnosis = if_else(str_detect(diagnosis, regex("type 2", ignore_case = TRUE)),
                             "Diabetes",
                             diagnosis)) %>% 
  filter(diagnosis %in% c("Depression", "Diabetes")) %>% 
    pivot_wider(
      id_cols = person_id,
    names_from = diagnosis,
    values_from = diagnosis,
    values_fn = length,  # Use length to count occurrences of each diagnosis
    values_fill = list(diagnosis = 0)  # Fill missing values with 0 (FALSE)
  ) %>% 
    mutate(across(starts_with("Depression"):starts_with("Diabetes"), ~ . != 0))

```

##Clean measurement covariates
```{r}
meas_covars <- dataset_67990468_measurement_df
meas_covars <- meas_covars %>%
  group_by(person_id) %>%
  #represent BMI as the average BMI across all visits available for an individual
  summarise(BMI = mean(value_as_number, na.rm = TRUE))
```

##Clean survey covariates
```{r}
surv_covars <- dataset_67990468_survey_df
surv_covars <- surv_covars %>% 
  pivot_wider(names_from = question,
              values_from = answer,
              values_fn = toString) %>% 
  #recode categorical variables, combining non-answer responses
  mutate(
    Income = factor(ifelse(`Income: Annual Income` %in% c("PMI: Prefer Not To Answer", "PMI: Skip"), 'Prefer not to answer', `Income: Annual Income`)),
    Smoking = as.numeric(ifelse(`Smoking: Number Of Years`=='PMI: Dont Know', 
                     mean(as.numeric(`Smoking: Number Of Years`), na.rm = T), 
                     ifelse(is.na(`Smoking: Number Of Years`), 0, `Smoking: Number Of Years`))),
    Race = gsub("What Race Ethnicity: ", "", `Race: What Race Ethnicity`, fixed = TRUE),
    Race = factor(ifelse(grepl(',', Race), 'Mixed',
                         ifelse(Race %in% c('PMI: Prefer Not To Answer', 'PMI: Skip'), 'Prefer not to answer', Race
                  ))),
    Education = factor(ifelse(`Education Level: Highest Grade` %in% c("PMI: Prefer Not To Answer", "PMI: Skip"), 'Prefer not to answer',`Education Level: Highest Grade`)),
    Alcohol = ifelse(`Alcohol: Drink Frequency Past Year` %in% c('PMI: Prefer Not To Answer', 'PMI: Skip', NA), 'Prefer not to answer', `Alcohol: Drink Frequency Past Year`)
    
  ) %>% 
  select(person_id, Income, Smoking, Race, Education, Alcohol)
levels(surv_covars$Race)
```

##Join all dataframes
```{r}
or_data <- cvd %>% 
  select(person_id, Alzheimers.Disease, everything()) %>% 
  left_join(condition_covars) %>% 
  left_join(meas_covars) %>% 
  left_join(surv_covars) %>% 
  mutate(Diabetes = ifelse(is.na(Diabetes), FALSE, Diabetes),
         Depression = ifelse(is.na(Depression), FALSE, Depression))
```

##Load smoking data
```{r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "Smoking" for domain "survey" and was generated for All of Us Controlled Tier Dataset v7
dataset_17097066_survey_sql <- paste("
    SELECT
        answer.person_id,
        answer.question,
        answer.answer  
    FROM
        `ds_survey` answer   
    WHERE
        (
            question_concept_id IN (1585857)
        )", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
survey_17097066_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "survey_17097066",
  "survey_17097066_*.csv")
message(str_glue('The data will be written to {survey_17097066_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_17097066_survey_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  survey_17097066_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {survey_17097066_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(survey = col_character(), question = col_character(), answer = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_17097066_survey_df <- read_bq_export_from_workspace_bucket(survey_17097066_path)

dim(dataset_17097066_survey_df)

head(dataset_17097066_survey_df, 5)

#replace smoking variable with one that provides a better parallel to UKB data
smoking_df <- dataset_17097066_survey_df %>% mutate(
  Smoking = factor(ifelse(answer %in% c('PMI: Prefer Not To Answer', 'PMI: Skip'), 'Prefer not to answer', answer))) %>% 
  select(person_id, Smoking)
levels(smoking_df$Smoking)
#add smoking to dataframe
or_data <- or_data %>% 
  select(-Smoking) %>% 
  left_join(smoking_df)

```
## Extract age data
``` {r}
library(tidyverse)
library(bigrquery)

# This query represents dataset "Demographics" for domain "person" and was generated for All of Us Controlled Tier Dataset v7
dataset_68029169_person_sql <- paste("
    SELECT
        person.person_id,
        p_gender_concept.concept_name as gender,
        person.birth_datetime as date_of_birth,
        p_race_concept.concept_name as race,
        p_ethnicity_concept.concept_name as ethnicity,
        p_sex_at_birth_concept.concept_name as sex_at_birth 
    FROM
        `person` person 
    LEFT JOIN
        `concept` p_gender_concept 
            ON person.gender_concept_id = p_gender_concept.concept_id 
    LEFT JOIN
        `concept` p_race_concept 
            ON person.race_concept_id = p_race_concept.concept_id 
    LEFT JOIN
        `concept` p_ethnicity_concept 
            ON person.ethnicity_concept_id = p_ethnicity_concept.concept_id 
    LEFT JOIN
        `concept` p_sex_at_birth_concept 
            ON person.sex_at_birth_concept_id = p_sex_at_birth_concept.concept_id", sep="")

# Formulate a Cloud Storage destination path for the data exported from BigQuery.
# NOTE: By default data exported multiple times on the same day will overwrite older copies.
#       But data exported on a different days will write to a new location so that historical
#       copies can be kept as the dataset definition is changed.
person_68029169_path <- file.path(
  Sys.getenv("WORKSPACE_BUCKET"),
  "bq_exports",
  Sys.getenv("OWNER_EMAIL"),
  strftime(lubridate::now(), "%Y%m%d"),  # Comment out this line if you want the export to always overwrite.
  "person_68029169",
  "person_68029169_*.csv")
message(str_glue('The data will be written to {person_68029169_path}. Use this path when reading ',
                 'the data into your notebooks in the future.'))

# Perform the query and export the dataset to Cloud Storage as CSV files.
# NOTE: You only need to run `bq_table_save` once. After that, you can
#       just read data from the CSVs in Cloud Storage.
bq_table_save(
  bq_dataset_query(Sys.getenv("WORKSPACE_CDR"), dataset_68029169_person_sql, billing = Sys.getenv("GOOGLE_PROJECT")),
  person_68029169_path,
  destination_format = "CSV")


# Read the data directly from Cloud Storage into memory.
# NOTE: Alternatively you can `gsutil -m cp {person_68029169_path}` to copy these files
#       to the Jupyter disk.
read_bq_export_from_workspace_bucket <- function(export_path) {
  col_types <- cols(gender = col_character(), race = col_character(), ethnicity = col_character(), sex_at_birth = col_character())
  bind_rows(
    map(system2('gsutil', args = c('ls', export_path), stdout = TRUE, stderr = TRUE),
        function(csv) {
          message(str_glue('Loading {csv}.'))
          chunk <- read_csv(pipe(str_glue('gsutil cat {csv}')), col_types = col_types, show_col_types = FALSE)
          if (is.null(col_types)) {
            col_types <- spec(chunk)
          }
          chunk
        }))
}
dataset_68029169_person_df <- read_bq_export_from_workspace_bucket(person_68029169_path)

dim(dataset_68029169_person_df)

head(dataset_68029169_person_df, 5)

table(dataset_68029169_person_df$sex_at_birth)

```

## Add predefined snippets, which will be used to calculated current age since birth
```{r}
lapply(c('viridis', 'ggthemes', 'skimr'),
       function(pkg_name) { if(! pkg_name %in% installed.packages()) { install.packages(pkg_name)} } )

library(viridis)    # A nice color scheme for plots.
library(ggthemes)   # Common themes to change the look and feel of plots.
library(scales)     # Graphical scales map data to aesthetics in plots.
library(skimr)      # Better summaries of data.
library(lubridate)  # Date library from the tidyverse.
library(tidyverse)  # Data wrangling packages.
library(bigrquery)  # Data extraction from Google BigQuery

## Plot setup.
theme_set(theme_bw(base_size = 14)) # Default theme for plots.

#' Returns a data frame with a y position and a label, for use annotating ggplot boxplots.
#'
#' @param d A data frame.
#' @return A data frame with column y as max and column label as length.
get_boxplot_fun_data <- function(df) {
  return(data.frame(y = max(df), label = stringr::str_c('N = ', length(df))))
}

# Use snippet 'add_age_to_demographics' to calculate the age of people in your demographics.
# It assumes the 'Setup' snippet has been executed.
# It also assumes that you got your demographics dataframe from Dataset Builder

# Note: This snippet calculates current age and does not take into account whether the person is already dead


## -----[ CHANGE THE DATAFRAME NAME(S) `YOUR_DATASET_NAME_person_df` TO MATCH YOURS FROM DATASET BUILDER] -----
dataset_68029169_person_df <- dataset_68029169_person_df %>%
                mutate_if(is.list, as.character) %>%
                mutate(Age = year(today()) - year(dataset_68029169_person_df$date_of_birth))
```

## Add age to cvd dataframe
```{r}
age_sex_df <- dataset_68029169_person_df %>% select(person_id, Age, sex_at_birth) %>% mutate(Sex = factor(sex_at_birth))

cvd <- or_data %>% left_join(age_sex_df) %>% 
  select(-person_id, -sex_at_birth)

levels(cvd$Sex)
levels(cvd$Sex)[levels(cvd$Sex) %in% c("I prefer not to answer", "PMI: Skip")] <- "Prefer not to answer"
```

##Get totals of each disease
```{r}
counts <- data.frame(lapply(cvd[1:12], sum))
```


##Perform Crosstabs on AD and CVD Counts
```{r}
library(dplyr)

# Your variables
vars <- names(cvd)[2:11]

# Build the results
results_list <- lapply(vars, function(var) {
  tmp <- table(cvd[[var]], cvd$Alzheimers.Disease)
  
  # Make sure tmp is a data.frame
  tmp_df <- as.data.frame.matrix(tmp)
  
  # Add the TRUE/FALSE label from the rownames
  tmp_df$CVD_status <- rownames(tmp_df)
  
  tmp_df$CVD_subtype <- var
  
  tmp_df
})

# Combine all
combined <- bind_rows(results_list)

# Arrange to have nice columns
combined <- combined %>%
  select(CVD_subtype, CVD_status, everything()) %>%
  rename(AD_No = `FALSE`, AD_Yes = `TRUE`)   # assuming Alzheimer's Disease is also logical

# Calculate totals and percentages
final_table <- combined %>%
  mutate(
    Total = AD_No + AD_Yes,
    Percent_AD_No = paste0(AD_No, " (", round(AD_No / Total * 100, 1), "%)"),
    Percent_AD_Yes = paste0(AD_Yes, " (", round(AD_Yes / Total * 100, 1), "%)"),
    CVD_status = paste0(ifelse(CVD_status, 'Yes', 'No'), " ", AD_No + AD_Yes)
  ) %>%
  select(CVD_subtype, CVD_status, Percent_AD_No, Percent_AD_Yes)

final_table

#remove pulmonary embolism (case count is too small for stable estimates)
cvd <- cvd %>% select(-Pulmonary.Embolism) %>% 
  filter(!is.na(Income)) %>% 
  #replace NA smoking values iwth prefer not to answer
  mutate(Smoking = fct_na_value_to_level(Smoking, level = "Prefer not to answer"))
  
#impute missing BMI values with median
cvd <- cvd %>% mutate(BMI = ifelse(is.na(BMI), median(BMI, na.rm = TRUE), BMI))
```

## Summary statistics for each group
```{r}
summarize_dataframe <- function(df) {
  results <- list()
  
  for (colname in names(df)) {
    column <- df[[colname]]
    
    if (is.numeric(column)) {
      # Mean and SD
      col_mean <- mean(column, na.rm = TRUE)
      col_sd <- sd(column, na.rm = TRUE)
      summary_text <- sprintf("%.2f (%.2f)", col_mean, col_sd)
      
      total <- length(column)
      na_count <- sum(is.na(column))
      na_percent <- (na_count / total) * 100
      na_text <- sprintf("%d (%.1f%%)", na_count, na_percent)
      
      results[[paste0(colname, "_summary")]] <- data.frame(
        Variable = colname,
        Level = "Mean (SD)",
        Summary = summary_text,
        stringsAsFactors = FALSE
      )
      
      results[[paste0(colname, "_na")]] <- data.frame(
        Variable = colname,
        Level = "NA",
        Summary = na_text,
        stringsAsFactors = FALSE
      )
      
    } else if (is.factor(column) || is.character(column) || is.logical(column)) {
      counts <- table(column, useNA = "ifany")
      percents <- prop.table(counts) * 100
      
      cat_df <- data.frame(
        Variable = colname,
        Level = names(counts),
        Summary = sprintf("%d (%.1f%%)", as.integer(counts), round(percents, 1)),
        stringsAsFactors = FALSE
      )
      
      results[[colname]] <- cat_df
    }
  }
  
  # Combine to a single data frame
  summary_df <- do.call(rbind, results)
  rownames(summary_df) <- NULL
  
  return(summary_df)
}

# Example usage:
covars <- summarize_dataframe(cvd[12:21])

#generate summary for participants with AD
covars_ad <- cvd[cvd$Alzheimers.Disease == TRUE,]
covars_ad <- summarize_dataframe(covars_ad[12:21])

#generate summary for participants without AD
covars_no_ad <- cvd[cvd$Alzheimers.Disease == FALSE,]
covars_no_ad <- summarize_dataframe(covars_no_ad[12:21])

#join summaries together into 1 dataframe
joined_covars <- covars %>% 
  full_join(covars_ad, by = c('Variable', 'Level')) %>% 
  full_join(covars_no_ad, by = c('Variable', 'Level'))

#write to excel file
write.csv(joined_covars, 'ad_covar_summary.csv', row.names = F)
```

## Run odds ratio test
```{r}
  #cvd data frame should have alzheimers in first column, then cvd subtypes
  #make sure eid is removed

  #create empty dataframe to store results
  output <- data.frame(cvd = character(10), est = numeric(10), 
                       lower.lim = numeric(10), upper.lim = numeric(10))
  
  #run regression for each subtype
  for(i in 1:10){
    #put name of disease in output dataframe
    output$cvd[i] <- names(cvd)[i+1]
    
    #save formula
    formula <- as.formula(paste0('`', names(cvd)[1], '` ~ `', names(cvd)[i+1], '` + ',
         paste(paste0('`', names(cvd)[13:ncol(cvd)], '`'), collapse = ' + '))
)
    
    #run regression
    mylogit <- glm(formula, family = 'binomial', data = cvd)
    
    #save logistic regression summary
    sum_mylogit <- summary(mylogit)
    
    #save odds ratio and confidence interval
    or_confint <- exp(cbind(OR = sum_mylogit$coefficients[2, 1], 
                            LL = sum_mylogit$coefficients[2, 1] 
                              - 1.96 * sum_mylogit$coefficients[2, 2],
                            UL = sum_mylogit$coefficients[2, 1] 
                              + 1.96 * sum_mylogit$coefficients[2, 2]))

    
    #save in output table
    output[i, 2] <- or_confint[1, 1]
    output[i, 3] <- or_confint[1, 2]
    output[i, 4] <- or_confint[1, 3]
    
  }
    
    #visualize with a forest plot
    plot <- ggplot(output, aes(x = reorder(cvd, est), y = est)) +
  geom_point(shape = 21, fill = "blue", size = 5) +
  geom_errorbar(aes(ymin = lower.lim, ymax = upper.lim), width = 0.15, linewidth = 0.75, color = 'black') +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  annotate("text", x = 10.35, y = 1.52, label = "Significance\nThreshold  ", 
           vjust = 1, hjust = 1.1, color = "red", size = 5) +  
  coord_flip() +
  labs(x = "CVD Subtype", y = "Odds Ratio (95% CI)", 
       title = "Forest Plot of Odds Ratios\nfor AD with CVD Subtypes",
       subtitle = 'All of Us Cohort')+
  theme(axis.text.x = element_text(size = 12),
        axis.text.y = element_text(size = 16),
        plot.title = element_text(hjust = 0.5, size = 30),
        plot.subtitle = element_text(hjust = 0.5),
        axis.title = element_text(size = 20),
        legend.text= element_text(size = 12),
        legend.title = element_text(size = 20),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_rect(fill = "white"))
    
  plot
    
        
    #save plot
    ggsave('or_plot_AoU.png', plot = plot,
           width = 9, height = 9)
    
    #save output data
    write.csv(output, 'AoU_or.csv')
```